# 一致性哈希算法

## 1. 背景

假设有这么一种场景：我们有三台缓存服务器分别为：node0、node1、node2，有3000万个缓存数据需要存储在这三台服务器组成的集群中，希望可以将这些数据均匀的缓存到三台机器上，你会想到什么方案呢？

我们可能首先想到的方案是：取模算法hash（key）%N，即：对缓存数据的key进行hash运算后取模，N是机器的数量；运算后的结果映射对应集群中的节点。首先对key进行hash计算后的结果对3取模，得到的结果一定是0、1或者2；然后映射对应的服务器node0、node1、node2，最后直接找对应的服务器存取数据即可。

看起来很完美！但是，在分布式集群系统的负载均衡实现上，这种模型在集群扩容和收缩时却有一定的局限性：因为在生产环境中根据业务量的大小，调整服务器数量是常有的事，而服务器数量N发生变化后hash（key）%N计算的结果也会随之变化！导致整个集群的缓存数据必须重新计算调整，进而导致大量缓存在同一时间失效，造成缓存的雪崩，最终导致整个缓存系统的不可用，这是不能接受的。为了解决优化上述情况，一致性哈希算法应运而生。

## 2. 一致性哈希简介

### 2.1 什么是一致性哈希？
一致性哈希(Consistent Hash)算法是1997年提出，是一种特殊的哈希算法，目的是解决分布式系统的数据分区问题：当分布式集群移除或者添加一个服务器时，必须尽可能小地改变已存在的服务请求与处理请求服务器之间的映射关系。

### 2.2 主要解决问题
我们知道，传统的按服务器节点数量取模在集群扩容和收缩时存在一定的局限性。而一致性哈希算法正好解决了简单哈希算法在分布式集群中存在的动态伸缩的问题。降低节点上下线的过程中带来的数据迁移成本，同时节点数量的变化与分片原则对于应用系统来说是无感的，使上层应用更专注于领域内逻辑的编写，使得整个系统架构能够动态伸缩，更加灵活方便。

### 2.3 使用场景
一致性哈希算法是分布式系统中的重要算法，使用场景也非常广泛。主要是是负载均衡、缓存数据分区等场景。一致性哈希应该是实现负载均衡的首选算法，它的实现比较灵活，既可以在客户端实现，也可以在中间件上实现，比如日常使用较多的缓存中间件memcached 使用的路由算法用的就是一致性哈希算法。

此外，其它的应用场景还有很多：
- RPC框架Dubbo用来选择服务提供者
- 分布式关系数据库分库分表：数据与节点的映射关系
- LVS负载均衡调度器

## 3. 一致性哈希的原理
### 3.1 算法原理
一个设计良好的分布式系统应该具有良好的单调性，即服务器的添加与移除不会造成大量的哈希重定位，而一致性哈希恰好可以解决这个问题。

其实，一致性哈希算法本质上也是一种取模算法。只不过前面介绍的取模算法是按服务器数量取模，而一致性哈希算法是`对固定值2^32取模`，这就使得一致性算法具备良好的单调性：不管集群中有多少个节点，只要key值固定，那所请求的服务器节点也同样是固定的。其算法的工作原理如下：
1. 一致性哈希算法将整个哈希值空间映射成一个虚拟的圆环，整个哈希空间的取值范围为0~2^32-1；
2. 计算各服务器节点的哈希值，并映射到哈希环上；
3. 将服务发来的数据请求使用哈希算法算出对应的哈希值；
4. 将计算的哈希值映射到哈希环上，同时沿圆环顺时针方向查找，遇到的第一台服务器就是所对应的处理请求服务器。
5. 当增加或者删除一台服务器时，受影响的数据仅仅是新添加或删除的服务器到其环空间中前一台的服务器（也就是顺着逆时针方向遇到的第一台服务器）之间的数据，其他都不会受到影响。

综上所述，一致性哈希算法对于节点的增减都只需重定位环空间中的一小部分数据，具有较好的容错性和可扩展性 。

### 3.2 深入剖析
#### 3.2.1 哈希环
首先，一致性哈希算法将整个哈希值空间映射成一个虚拟的圆环。整个哈希空间的取值范围为0~2^32-1，按顺时针方向开始从0~2^32-1排列，最后的节点2^32-1在0开始位置重合，形成一个虚拟的圆环。如下图所示：

![hash-ring](/file/img/consistent_hash_01_ring.webp)

#### 3.2.2 服务器映射到哈希环
接下来，将服务器节点映射到哈希环上对应的位置。我们可以对服务器IP地址进行哈希计算，哈希计算后的结果对2^32取模，结果一定是一个0到2^32-1之间的整数。最后将这个整数映射在哈希环上，整数的值就代表了一个服务器节点的在哈希环上的位置。即：`hash（服务器ip）% 2^32`。下面我们依次将node0、node1、node2三个缓存服务器映射到哈希环上，如下图所示：

![hash-node](/file/img/consistent_hash_02_node.webp)

#### 3.2.3 对象key映射到服务器
当服务器接收到数据请求时，首先需要计算请求Key的哈希值；然后将计算的哈希值映射到哈希环上的具体位置；接下来，从这个位置沿着哈希环顺时针查找，遇到的第一个节点就是key对应的节点；最后，将请求发送到具体的服务器节点执行数据操作。
假设我们有“key-01：张三”、“key-02：李四”、“key-03：王五”三条缓存数据。经过哈希算法计算后，映射到哈希环上的位置如下图所示：

![hash-key](/file/img/consistent_hash_03_key.webp)

如上图所示，通过哈希计算后，key-01顺时针寻找将找到node0，key-02顺时针寻找将找到node1，key-03顺时针寻找将找到node2。最后，请求找到的服务器节点执行具体的业务操作。以上便是一致性哈希算法的工作原理。

## 4. 服务器扩容&缩容
前面介绍了一致性哈希算法的工作原理，那么，一致性哈希算法如何避免服务器动态伸缩的问题的呢？

### 4.1 服务器缩容
服务器缩容就是减少集群中服务器节点的数量或是集群中某个节点故障。假设，集群中的某个节点故障，原本映射到该节点的请求，会找到哈希环中的下一个节点，数据也同样被重新分配至下一个节点，其它节点的数据和请求不受任何影响。这样就确保节点发生故障时，集群能保持正常稳定。如下图所示：

![hash-shrink](/file/img/consistent_hash_04_shrink.webp)

如上图所示：节点node2发生故障时，数据key-01和key-02不会受到影响，只有key-03的请求被重定位到node0。在一致性哈希算法中，如果某个节点宕机不可用了，那么受影响的数据仅仅是会寻址到此节点和前一节点之间的数据。其他哈希环上的数据不会受到影响。

### 4.2 服务器扩容
服务器扩容就是集群中需要增加一个新的数据节点，假设，由于需要缓存的数据量太大，必须对集群进行扩容增加一个新的数据节点。此时，只需要计算新节点的哈希值并将新的节点加入到哈希环中，然后将哈希环中从上一个节点到新节点的数据映射到新的数据节点即可。其他节点数据不受影响，具体如下图所示：

![hash-expand](/file/img/consistent_hash_05_expand.webp)

如上图所示，加入新的node3节点后，key-01、key-02、key-03不受影响，只有key-04的寻址被重定位到新节点node3，受影响的数据仅仅是会寻址到新节点和前一节点之间的数据。

通过一致性哈希算法，集群扩容或缩容时，只需要重新定位哈希环空间内的一小部分数据。其他数据保持不变。**当节点数越多的时候，使用哈希算法时，需要迁移的数据就越多，使用一致哈希时，需要迁移的数据就越少**。所以，一致哈希算法具有较好的容错性和可扩展性。

## 5. 数据倾斜与虚拟节点
### 5.1 什么是数据倾斜？
由于哈希计算的随机性，导致一致性哈希算法存在一个致命问题：数据倾斜，也就是说大多数访问请求都会集中少量几个节点的情况。特别是节点太少情况下，容易因为节点分布不均匀造成数据访问的冷热不均。这就失去了集群和负载均衡的意义。如下图所示：

![hash-skew](/file/img/consistent_hash_06_skew.webp)

如上图所示，key-1、key-2、key-3可能被映射到同一个节点node0上。导致node0负载过大，而node1和node2却很空闲的情况。这有可能导致个别服务器数据和请求压力过大和崩溃，进而引起集群的崩溃。

### 5.2 如何解决数据倾斜？
为了解决数据倾斜的问题，一致性哈希算法引入了虚拟节点机制，即对每一个物理服务节点映射多个虚拟节点，将这些虚拟节点计算哈希值并映射到哈希环上，当请求找到某个虚拟节点后，将被重新映射到具体的物理节点。**虚拟节点越多，哈希环上的节点就越多，数据分布就越均匀，从而避免了数据倾斜的问题**。
说起来可能比较复杂，一句话概括起来就是：原有的节点、数据定位的哈希算法不变，只是多了一步虚拟节点到实际节点的映射。具体如下图所示：

![hash-virtual](/file/img/consistent_hash_07_virtual.webp)

如上图所示，我们可以在服务器ip或主机名的后面增加编号来实现，将全部的虚拟节点加入到哈希环中，增加了节点后，数据在哈希环上的分布就相对均匀了。当有访问请求寻址到node0-1这个虚拟节点时，将被重新映射到物理节点node0。

## 6. 一致性Hash算法实现

```go
package consistentHash

import (
	"fmt"
	"hash/crc32"
	"sort"
)

type Node struct {
	IP                  string
	VirtualNodeHashList []uint32
	Data                map[string]string // to store data.
}

func NewNode(ip string, VirtualNodeCount int) *Node {
	var list []uint32
	for i := 1; i <= VirtualNodeCount; i++ {
		list = append(list, Hash(fmt.Sprintf("%s : %d", ip, i)))
	}
	return &Node{
		IP:                  ip,
		VirtualNodeHashList: list,
		Data:                make(map[string]string),
	}
}

func (n *Node) AddCacheItem(key, value string) {
	n.Data[key] = value
}

func (n *Node) GetCacheItem(key string) string {
	return n.Data[key]
}

func (n *Node) RemoveCacheItem(key string) {
	delete(n.Data, key)
}

type ConsistentHash struct {
	SortedVirtualNodes []uint32
	HashRing           map[uint32]*Node
}

func NewConsistentHash() *ConsistentHash {
	return &ConsistentHash{
		SortedVirtualNodes: make([]uint32, 0),
		HashRing:           make(map[uint32]*Node),
	}
}

func (ch *ConsistentHash) AddNode(n *Node) {
	ch.SortedVirtualNodes = append(ch.SortedVirtualNodes, n.VirtualNodeHashList...)
	for _, hash := range n.VirtualNodeHashList {
		ch.HashRing[hash] = n
	}
	sort.Slice(ch.SortedVirtualNodes, func(i, j int) bool {
		return ch.SortedVirtualNodes[i] < ch.SortedVirtualNodes[j]
	})
}

func (ch *ConsistentHash) RemoveNode(n *Node) {
	// remove virtual node from hash ring and sorted virtual node list
	sort.Slice(n.VirtualNodeHashList, func(i, j int) bool {
		return n.VirtualNodeHashList[i] < n.VirtualNodeHashList[j]
	})
	ch.SortedVirtualNodes = removeSubFromSorted(ch.SortedVirtualNodes, n.VirtualNodeHashList)
	for _, hash := range n.VirtualNodeHashList {
		delete(ch.HashRing, hash)
	}
}

func removeSubFromSorted(mainArr []uint32, subArr []uint32) []uint32 {
	subIndex := 0 // index for subArr
	result := make([]uint32, 0)
	for _, val := range mainArr {
		if subIndex < len(subArr) && val == subArr[subIndex] {
			// Skip the current element if it matches the sub-array element
			subIndex++
		} else {
			// Otherwise, add it to the result
			result = append(result, val)
		}
	}
	return result
}

func (ch *ConsistentHash) UpdateDataAfterRemove(n *Node) {
	for k, v := range n.Data {
		ch.Put(k, v)
	}
	n.Data = make(map[string]string)
}

func (ch *ConsistentHash) Get(key string) string {
	n := ch.FindMatchNode(key)
	return n.GetCacheItem(key)
}

func (ch *ConsistentHash) Put(key, value string) {
	n := ch.FindMatchNode(key)
	n.AddCacheItem(key, value)
}

func (ch *ConsistentHash) Evict(key string) {
	ch.FindMatchNode(key).RemoveCacheItem(key)
}

func (ch *ConsistentHash) FindMatchNode(key string) *Node {
	hash := Hash(key)
	i := ch.getPosition(hash)
	return ch.HashRing[ch.SortedVirtualNodes[i]]
}

func (ch *ConsistentHash) getPosition(hash uint32) int {
	i := sort.Search(len(ch.SortedVirtualNodes), func(i int) bool { return hash < ch.SortedVirtualNodes[i] })
	if hash == 4268915967 {
		fmt.Println("ix :", i)
	}
	if i < len(ch.SortedVirtualNodes) {
		return i
	}
	return 0
}

func Hash(item string) uint32 {
	return crc32.ChecksumIEEE([]byte(item))
}
```

```go
package main

import (
	"fmt"
	"math"
	"strconv"

	"Z_Interview/consistentHash"
)

func main() {
	testCount := 1000000 // 100w requests

	// test 1: 模拟负载均衡，virtual node数量的影响
	virtualNodeList := []int{100, 150, 200}
	nodeNum := 10 // 10 servers
	for _, virtualNode := range virtualNodeList {
		ch := consistentHash.NewConsistentHash()
		distributeMap := make(map[string]int64) // key is IP, value is request num.
		for i := 1; i <= nodeNum; i++ {
			serverName := "172.17.0." + strconv.Itoa(i)
			n := consistentHash.NewNode(serverName, virtualNode)
			ch.AddNode(n)
		}
		//测试100W个请求的分布
		for i := 0; i < testCount; i++ {
			testName := "request"
			serverName := ch.FindMatchNode(testName + strconv.Itoa(i)).IP
			distributeMap[serverName] += 1
		}

		var values []float64
		fmt.Printf("%d Server Nodes,  %d Virtual Nodes per Node, mock %d requests.\n", nodeNum, virtualNode, testCount)
		for k, v := range distributeMap {
			fmt.Printf("server: %s, requests handled: %d\n", k, v)
			values = append(values, float64(v))
		}
		fmt.Printf("标准差:%f\n\n", getStandardDeviation(values))
	}
	fmt.Println("======================================")

	// test 2: 模拟负载均衡，delete node
	n1 := consistentHash.NewNode("172.17.0.1", 200)
	n2 := consistentHash.NewNode("172.17.0.2", 200)
	n3 := consistentHash.NewNode("172.17.0.3", 200)
	ch1 := consistentHash.NewConsistentHash()
	ch1.AddNode(n1)
	ch1.AddNode(n2)
	ch1.AddNode(n3)
	distributeMap := make(map[string]int64)
	for i := 0; i < testCount; i++ {
		testName := "testRequest"
		serverName := ch1.FindMatchNode(testName + strconv.Itoa(i)).IP
		distributeMap[serverName] += 1
	}
	for k, v := range distributeMap {
		fmt.Printf("server: %s, reqeusts handled: %d\n", k, v)
	}
	ch1.RemoveNode(n3)
	for i := 0; i < testCount; i++ {
		testName := "anotherRequest"
		serverName := ch1.FindMatchNode(testName + strconv.Itoa(i)).IP
		distributeMap[serverName] += 1
	}
	for k, v := range distributeMap {
		fmt.Printf("server: %s, reqeusts handled: %d\n", k, v)
	}
	fmt.Println("======================================")

	// test 3: store data.
	ch2 := consistentHash.NewConsistentHash()
	ch2.AddNode(n1)
	ch2.AddNode(n2)
	ch2.AddNode(n3)
	for i := 0; i < testCount; i++ {
		testName := "testData"
		ch2.Put(testName+strconv.Itoa(i), strconv.Itoa(i))
	}
	fmt.Printf("server: %s, data size : %d\n", n1.IP, len(n1.Data))
	fmt.Printf("server: %s, data size : %d\n", n2.IP, len(n2.Data))
	fmt.Printf("server: %s, data size : %d\n", n3.IP, len(n3.Data))

	// remove node 3
	ch2.RemoveNode(n3)
	ch2.UpdateDataAfterRemove(n3)

	fmt.Printf("server: %s, data size : %d\n", n1.IP, len(n1.Data))
	fmt.Printf("server: %s, data size : %d\n", n2.IP, len(n2.Data))
	fmt.Printf("server: %s, data size : %d\n", n3.IP, len(n3.Data))

	// delete
	for i := 0; i < testCount; i++ {
		testName := "testData"
		ch2.Evict(testName + strconv.Itoa(i))
	}
	fmt.Printf("server: %s, data size : %d\n", n1.IP, len(n1.Data))
	fmt.Printf("server: %s, data size : %d\n", n2.IP, len(n2.Data))
	fmt.Printf("server: %s, data size : %d\n", n3.IP, len(n3.Data))

	fmt.Println("======================================")

	for k, v := range n1.Data {
		fmt.Printf("key: %s, value: %s\n", k, v)
	}

	for k, v := range n2.Data {
		fmt.Printf("key: %s, value: %s\n", k, v)
	}

	ch2.Evict("testData430")
	fmt.Printf("server: %s, data size : %d\n", n1.IP, len(n1.Data))
}

// 获取标准差
func getStandardDeviation(list []float64) float64 {
	var total float64
	for _, item := range list {
		total += item
	}
	//平均值
	avg := total / float64(len(list))
	var dTotal float64
	for _, value := range list {
		dValue := value - avg
		dTotal += dValue * dValue
	}
	return math.Sqrt(dTotal / avg)
}
```

上面的实现中：
1. 如果作为负载均衡，没有问题。增加节点和减少节点，都可以找到相应的节点。
2. 如果作为缓存，只考虑了删除节点时，缓存数据同步到了其他节点。
3. 如果增加节点，并没有进一步更新缓存数据，会造成缓存数据无法命中的情况，比如缓存key在节点1，增加节点4后，key命中了节点4。导致DB查询并更新缓存，如果再次删除节点4，再次命中节点1，会造成数据不一致（DB已经更新）。


一下是别人的参考代码：只实现了固定节点数量。
```go
package JiKe

import (
	"errors"
	"hash/crc32"
	"sort"
	"strconv"
	"sync"
)

type Consistent struct {
	//排序的hash虚拟结点
	hashSortedNodes []uint32
	//虚拟结点对应的结点信息
	circle map[uint32]string
	//已绑定的结点
	nodes map[string]bool
	//map读写锁
	sync.RWMutex
	//虚拟结点数
	virtualNodeCount int
}

func (c *Consistent) hashKey(key string) uint32 {
	return crc32.ChecksumIEEE([]byte(key))
}

func (c *Consistent) Add(node string, virtualNodeCount int) error {
	if node == "" {
		return nil
	}
	c.Lock()
	defer c.Unlock()

	if c.circle == nil {
		c.circle = map[uint32]string{}
	}
	if c.nodes == nil {
		c.nodes = map[string]bool{}
	}

	if _, ok := c.nodes[node]; ok {
		return errors.New("node already existed")
	}
	c.nodes[node] = true
	//增加虚拟结点
	for i := 0; i < virtualNodeCount; i++ {
		virtualKey := c.hashKey(node + strconv.Itoa(i))
		c.circle[virtualKey] = node
		c.hashSortedNodes = append(c.hashSortedNodes, virtualKey)
	}

	//虚拟结点排序
	sort.Slice(c.hashSortedNodes, func(i, j int) bool {
		return c.hashSortedNodes[i] < c.hashSortedNodes[j]
	})

	return nil
}

func (c *Consistent) GetNode(key string) string {
	c.RLock()
	defer c.RUnlock()

	hash := c.hashKey(key)
	i := c.getPosition(hash)

	return c.circle[c.hashSortedNodes[i]]
}

func (c *Consistent) getPosition(hash uint32) int {
	i := sort.Search(len(c.hashSortedNodes), func(i int) bool { return c.hashSortedNodes[i] >= hash })

	if i < len(c.hashSortedNodes) {
		if i == len(c.hashSortedNodes)-1 {
			return 0
		} else {
			return i
		}
	} else {
		return len(c.hashSortedNodes) - 1
	}
}

//test
func Test_ConsistentHash(t *testing.T) {
	virtualNodeList := []int{100, 150, 200}
  //测试10台服务器
	nodeNum := 10
  //测试数据量100W
	testCount := 1000000
	for _, virtualNode := range virtualNodeList {
		consistentHash := &Consistent{}
		distributeMap := make(map[string]int64)
		for i := 1; i <= nodeNum; i++ {
			serverName := "172.17.0." + strconv.Itoa(i)
			consistentHash.Add(serverName, virtualNode)
			distributeMap[serverName] = 0
		}
		//测试100W个数据分布
		for i := 0; i < testCount; i++ {
			testName := "testName"
			serverName := consistentHash.GetNode(testName + strconv.Itoa(i))
			distributeMap[serverName] = distributeMap[serverName] + 1
		}

		var keys []string
		var values []float64
		for k, v := range distributeMap {
			keys = append(keys, k)
			values = append(values, float64(v))
		}
		sort.Strings(keys)
		fmt.Printf("####测试%d个结点,一个结点有%d个虚拟结点,%d条测试数据\n", nodeNum, virtualNode, testCount)
		for _, k := range keys {
			fmt.Printf("服务器地址:%s 分布数据数:%d\n", k, distributeMap[k])
		}
		fmt.Printf("标准差:%f\n\n", getStandardDeviation(values))
	}
}

//获取标准差
func getStandardDeviation(list []float64) float64 {
	var total float64
	for _, item := range list {
		total += item
	}
	//平均值
	avg := total / float64(len(list))

	var dTotal float64
	for _, value := range list {
		dValue := value - avg
		dTotal += dValue * dValue
	}

	return math.Sqrt(dTotal / avg)
}
```
